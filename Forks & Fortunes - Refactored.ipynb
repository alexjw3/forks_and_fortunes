{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç¥üí∞ Forks & Fortunes: Bay Area Restaurant Wealth Analysis\n",
    "\n",
    "This notebook analyzes the relationship between real estate wealth and restaurant density across Bay Area cities.\n",
    "\n",
    "**Key Questions:**\n",
    "- Do wealthy areas have proportionally more or fewer restaurants?\n",
    "- Which affluent areas might be \"under-served\" by restaurants?\n",
    "- How does restaurant density correlate with property values and population?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install census us folium geopy tqdm --quiet\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from census import Census\n",
    "from us import states\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîê Configuration - UPDATE THESE WITH YOUR API KEYS\n",
    "class Config:\n",
    "    # API Keys\n",
    "    CENSUS_API_KEY = '5f0c3b943ebcb5cafdd415749ed46d0556c637e7'\n",
    "    GOOGLE_API_KEY = \"AIzaSyCc16HaIw9lDe_sm8Fcr0TjSEnQzaEeiP4\"\n",
    "    \n",
    "    # File paths\n",
    "    DATA_DIR = \"./data\"\n",
    "    MAPS_DIR = \"./maps\"\n",
    "    ZILLOW_FILE = \"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"\n",
    "    \n",
    "    # Analysis parameters\n",
    "    RESTAURANT_SEARCH_RADIUS_KM = 5\n",
    "    SEARCH_STEP_KM = 0.25\n",
    "    GOOGLE_API_DELAY = 1.2  # seconds between API calls\n",
    "    \n",
    "    # Bay Area ZIP codes\n",
    "    BAY_AREA_ZIPS = [\n",
    "        # San Jose (951xx)\n",
    "        '95110', '95112', '95113', '95116', '95117', '95118', '95119', '95120', '95121', '95122',\n",
    "        '95123', '95124', '95125', '95126', '95127', '95128', '95129', '95130', '95131', '95132',\n",
    "        '95133', '95134', '95135', '95136', '95138', '95139', '95148',\n",
    "        # Santa Clara, Sunnyvale\n",
    "        '95050', '95051', '95054', '94085', '94086', '94087', '94089',\n",
    "        # Mountain View, Los Altos\n",
    "        '94040', '94041', '94043', '94022', '94024',\n",
    "        # Palo Alto & Stanford\n",
    "        '94301', '94303', '94304', '94305',\n",
    "        # Menlo Park, Atherton, Redwood City\n",
    "        '94025', '94027', '94028', '94063', '94061', '94062', '94065',\n",
    "        # Belmont, San Carlos, Foster City\n",
    "        '94070', '94404', '94002',\n",
    "        # San Mateo, Burlingame, Hillsborough\n",
    "        '94401', '94402', '94403', '94010',\n",
    "        # Millbrae, San Bruno, South SF, Daly City\n",
    "        '94030', '94066', '94080', '94014', '94015',\n",
    "        # San Francisco (941xx)\n",
    "        '94102', '94103', '94104', '94105', '94107', '94108', '94109', '94110',\n",
    "        '94111', '94112', '94114', '94115', '94116', '94117', '94118', '94121',\n",
    "        '94122', '94123', '94124', '94127', '94129', '94130', '94131', '94132',\n",
    "        '94133', '94134', '94158',\n",
    "        # Oakland (946xx)\n",
    "        '94601', '94602', '94603', '94605', '94606', '94607', '94608', '94609',\n",
    "        '94610', '94611', '94612', '94618', '94619', '94621',\n",
    "        # Berkeley (947xx)\n",
    "        '94702', '94703', '94704', '94705', '94706', '94707', '94708', '94709', '94710',\n",
    "        # East Bay\n",
    "        '94536', '94538', '94539', '94541', '94542', '94544', '94545', '94546',\n",
    "        '94552', '94555', '94560', '94577', '94578', '94579', '94580', '94586', '94587',\n",
    "        # Marin County (949xx)\n",
    "        '94901', '94903', '94904', '94920', '94925', '94930', '94939', '94941',\n",
    "        '94945', '94947', '94949', '94960', '94965'\n",
    "    ]\n",
    "    \n",
    "    # Cities to analyze\n",
    "    CITIES_TO_ANALYZE = [\n",
    "        # Peninsula\n",
    "        \"Portola Valley\", \"Atherton\", \"Menlo Park\", \"Palo Alto\", \"Los Altos\", \"Mountain View\",\n",
    "        \"Redwood City\", \"San Carlos\", \"Belmont\", \"Foster City\", \"San Mateo\", \"Hillsborough\",\n",
    "        \"Burlingame\", \"Millbrae\", \"San Bruno\", \"South San Francisco\", \"Daly City\",\n",
    "        # San Francisco\n",
    "        \"San Francisco\"\n",
    "    ]\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(Config.DATA_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MAPS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìÅ Data directory: {Config.DATA_DIR}\")\n",
    "print(f\"üó∫Ô∏è Maps directory: {Config.MAPS_DIR}\")\n",
    "print(f\"üèôÔ∏è Cities to analyze: {len(Config.CITIES_TO_ANALYZE)}\")\n",
    "print(f\"üìÆ ZIP codes: {len(Config.BAY_AREA_ZIPS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Data Collection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    \"\"\"Handles all data collection from various APIs and sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.census = Census(config.CENSUS_API_KEY)\n",
    "        self.geolocator = Nominatim(user_agent=\"forks-fortunes-analysis\")\n",
    "        \n",
    "        # Census variables mapping\n",
    "        self.census_variables = {\n",
    "            'B25077_001E': 'median_home_value',\n",
    "            'B25001_001E': 'housing_units',\n",
    "            'B19013_001E': 'median_income',\n",
    "            'B01003_001E': 'population'\n",
    "        }\n",
    "    \n",
    "    def collect_census_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Collect Census ACS data for Bay Area ZIP codes\"\"\"\n",
    "        print(\"üìä Collecting Census data...\")\n",
    "        \n",
    "        zip_data = []\n",
    "        failed_zips = []\n",
    "        \n",
    "        for zip_code in tqdm(self.config.BAY_AREA_ZIPS, desc=\"ZIP codes\"):\n",
    "            try:\n",
    "                res = self.census.acs5.get(\n",
    "                    list(self.census_variables.keys()),\n",
    "                    {'for': f'zip code tabulation area:{zip_code}'}\n",
    "                )\n",
    "                if res:\n",
    "                    entry = res[0]\n",
    "                    entry['zip'] = zip_code\n",
    "                    zip_data.append(entry)\n",
    "            except Exception as e:\n",
    "                failed_zips.append(zip_code)\n",
    "                continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(zip_data)\n",
    "        df.rename(columns=self.census_variables, inplace=True)\n",
    "        \n",
    "        # Convert numeric fields\n",
    "        for col in self.census_variables.values():\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df = df[['zip'] + list(self.census_variables.values())]\n",
    "        \n",
    "        print(f\"‚úÖ Collected data for {len(df)} ZIP codes\")\n",
    "        if failed_zips:\n",
    "            print(f\"‚ö†Ô∏è Failed for {len(failed_zips)} ZIP codes: {failed_zips[:5]}...\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load_zillow_data(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and process Zillow ZHVI data\"\"\"\n",
    "        print(\"üè† Loading Zillow ZHVI data...\")\n",
    "        \n",
    "        try:\n",
    "            zillow_df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Filter for California\n",
    "            zillow_df = zillow_df[zillow_df['State'] == 'CA'].copy()\n",
    "            \n",
    "            # Get most recent month's ZHVI\n",
    "            date_columns = [col for col in zillow_df.columns if '-' in col and len(col) == 7]\n",
    "            if date_columns:\n",
    "                latest_month = sorted(date_columns)[-1]\n",
    "                zillow_df['zhvi_latest'] = pd.to_numeric(zillow_df[latest_month], errors='coerce')\n",
    "            \n",
    "            # Clean up\n",
    "            zillow_df = zillow_df[['RegionName', 'City', 'zhvi_latest']].copy()\n",
    "            zillow_df.rename(columns={'RegionName': 'zip'}, inplace=True)\n",
    "            zillow_df['zip'] = zillow_df['zip'].astype(str).str.zfill(5)\n",
    "            \n",
    "            # Filter for Bay Area ZIPs\n",
    "            zillow_df = zillow_df[zillow_df['zip'].isin(self.config.BAY_AREA_ZIPS)].copy()\n",
    "            zillow_df = zillow_df.sort_values('zhvi_latest', ascending=False)\n",
    "            \n",
    "            print(f\"‚úÖ Loaded Zillow data for {len(zillow_df)} Bay Area ZIP codes\")\n",
    "            return zillow_df\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Zillow file not found: {filepath}\")\n",
    "            print(\"Please download the Zillow ZHVI data and update the file path in Config.ZILLOW_FILE\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_city_center(self, city_name: str, state_code: str = 'CA') -> tuple:\n",
    "        \"\"\"Get city center coordinates\"\"\"\n",
    "        location = self.geolocator.geocode(f\"{city_name}, {state_code}\")\n",
    "        if not location:\n",
    "            raise ValueError(f\"Could not find center for {city_name}\")\n",
    "        return location.latitude, location.longitude\n",
    "\n",
    "# Initialize data collector\n",
    "data_collector = DataCollector(Config)\n",
    "print(\"‚úÖ Data collector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçΩÔ∏è Restaurant Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestaurantAnalyzer:\n",
    "    \"\"\"Handles restaurant data collection and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.geolocator = Nominatim(user_agent=\"restaurant-analyzer\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def haversine_distance_km(lat1: float, lng1: float, lat2: float, lng2: float) -> float:\n",
    "        \"\"\"Calculate distance between two points in km\"\"\"\n",
    "        R = 6371  # Earth's radius in km\n",
    "        phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "        delta_phi = math.radians(lat2 - lat1)\n",
    "        delta_lambda = math.radians(lng2 - lng1)\n",
    "        a = (math.sin(delta_phi / 2.0) ** 2 + \n",
    "             math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2.0) ** 2)\n",
    "        return R * (2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)))\n",
    "    \n",
    "    def get_restaurants_near_point(self, lat: float, lng: float, \n",
    "                                 radius: int = 1000, max_pages: int = 3) -> list:\n",
    "        \"\"\"Get restaurants near a specific point using Google Places API\"\"\"\n",
    "        url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "        params = {\n",
    "            'location': f\"{lat},{lng}\",\n",
    "            'radius': radius,\n",
    "            'type': 'restaurant',\n",
    "            'key': self.config.GOOGLE_API_KEY\n",
    "        }\n",
    "        \n",
    "        restaurant_points = []\n",
    "        response = requests.get(url, params=params).json()\n",
    "        \n",
    "        if response.get(\"status\") != \"OK\":\n",
    "            if response.get(\"status\") == \"OVER_QUERY_LIMIT\":\n",
    "                print(\"‚ö†Ô∏è Google API quota exceeded\")\n",
    "            return []\n",
    "        \n",
    "        # Process first page\n",
    "        restaurant_points.extend([\n",
    "            (r['geometry']['location']['lat'], \n",
    "             r['geometry']['location']['lng'], \n",
    "             r.get('name', 'Unnamed'))\n",
    "            for r in response.get('results', [])\n",
    "        ])\n",
    "        \n",
    "        # Process additional pages\n",
    "        next_page_token = response.get('next_page_token')\n",
    "        page_count = 1\n",
    "        \n",
    "        while next_page_token and page_count < max_pages:\n",
    "            time.sleep(2)  # Required delay for next page token\n",
    "            next_params = {\"pagetoken\": next_page_token, \"key\": self.config.GOOGLE_API_KEY}\n",
    "            next_response = requests.get(url, params=next_params).json()\n",
    "            \n",
    "            restaurant_points.extend([\n",
    "                (r['geometry']['location']['lat'], \n",
    "                 r['geometry']['location']['lng'], \n",
    "                 r.get('name', 'Unnamed'))\n",
    "                for r in next_response.get('results', [])\n",
    "            ])\n",
    "            \n",
    "            next_page_token = next_response.get('next_page_token')\n",
    "            page_count += 1\n",
    "        \n",
    "        return restaurant_points\n",
    "    \n",
    "    def get_restaurants_within_radius(self, city_name: str) -> tuple:\n",
    "        \"\"\"Get all restaurants within radius of city center using grid sweep\"\"\"\n",
    "        try:\n",
    "            lat_c, lng_c = data_collector.get_city_center(city_name)\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ùå {e}\")\n",
    "            return [], (None, None)\n",
    "        \n",
    "        # Calculate grid parameters\n",
    "        delta_deg = self.config.SEARCH_STEP_KM / 111  # Rough conversion km to degrees\n",
    "        steps = int(self.config.RESTAURANT_SEARCH_RADIUS_KM / self.config.SEARCH_STEP_KM)\n",
    "        \n",
    "        all_restaurants = []\n",
    "        search_points = 0\n",
    "        \n",
    "        # Grid sweep\n",
    "        for dx in range(-steps, steps + 1):\n",
    "            for dy in range(-steps, steps + 1):\n",
    "                lat = lat_c + dy * delta_deg\n",
    "                lng = lng_c + dx * delta_deg\n",
    "                \n",
    "                # Check if point is within radius\n",
    "                dist = self.haversine_distance_km(lat_c, lng_c, lat, lng)\n",
    "                if dist <= self.config.RESTAURANT_SEARCH_RADIUS_KM:\n",
    "                    points = self.get_restaurants_near_point(lat, lng, radius=1000)\n",
    "                    all_restaurants.extend(points)\n",
    "                    search_points += 1\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(self.config.GOOGLE_API_DELAY)\n",
    "        \n",
    "        # Deduplicate based on coordinates (with small tolerance for GPS variance)\n",
    "        unique_restaurants = []\n",
    "        seen_coords = set()\n",
    "        \n",
    "        for lat, lng, name in all_restaurants:\n",
    "            # Round to 5 decimal places for deduplication\n",
    "            coord_key = (round(lat, 5), round(lng, 5))\n",
    "            if coord_key not in seen_coords:\n",
    "                seen_coords.add(coord_key)\n",
    "                unique_restaurants.append((lat, lng, name))\n",
    "        \n",
    "        print(f\"üîç {city_name}: Searched {search_points} points, found {len(unique_restaurants)} unique restaurants\")\n",
    "        return unique_restaurants, (lat_c, lng_c)\n",
    "    \n",
    "    def create_restaurant_map(self, city_name: str, restaurants: list, center: tuple) -> folium.Map:\n",
    "        \"\"\"Create an interactive map showing restaurants\"\"\"\n",
    "        if center[0] is None:\n",
    "            return None\n",
    "            \n",
    "        # Create map\n",
    "        m = folium.Map(location=center, zoom_start=13)\n",
    "        \n",
    "        # Add restaurants as clustered markers\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "        for lat, lng, name in restaurants:\n",
    "            folium.Marker(\n",
    "                location=[lat, lng],\n",
    "                popup=name,\n",
    "                tooltip=name\n",
    "            ).add_to(marker_cluster)\n",
    "        \n",
    "        # Add search radius circle\n",
    "        folium.Circle(\n",
    "            location=center,\n",
    "            radius=self.config.RESTAURANT_SEARCH_RADIUS_KM * 1000,  # Convert to meters\n",
    "            color=\"blue\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.1,\n",
    "            popup=f\"{self.config.RESTAURANT_SEARCH_RADIUS_KM}km search radius\"\n",
    "        ).add_to(m)\n",
    "        \n",
    "        return m\n",
    "\n",
    "# Initialize restaurant analyzer\n",
    "restaurant_analyzer = RestaurantAnalyzer(Config)\n",
    "print(\"‚úÖ Restaurant analyzer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analysis & Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisUtils:\n",
    "    \"\"\"Utilities for data analysis and visualization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_currency(x):\n",
    "        \"\"\"Format number as currency\"\"\"\n",
    "        return f\"${x:,.0f}\" if pd.notnull(x) else \"N/A\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_large_number(x, suffix=''):\n",
    "        \"\"\"Format large numbers with appropriate suffixes\"\"\"\n",
    "        if pd.isnull(x):\n",
    "            return \"N/A\"\n",
    "        if x >= 1e9:\n",
    "            return f\"{x/1e9:.2f}B{suffix}\"\n",
    "        elif x >= 1e6:\n",
    "            return f\"{x/1e6:.1f}M{suffix}\"\n",
    "        elif x >= 1e3:\n",
    "            return f\"{x/1e3:.0f}K{suffix}\"\n",
    "        else:\n",
    "            return f\"{x:.0f}{suffix}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_ranking_plot(df, x_col, y_col, title, x_label, top_n=15):\n",
    "        \"\"\"Create a horizontal bar plot for rankings\"\"\"\n",
    "        plot_df = df.nlargest(top_n, x_col).copy()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(plot_df[y_col], plot_df[x_col], color='skyblue', edgecolor='navy')\n",
    "        \n",
    "        # Format x-axis as currency if it's a value column\n",
    "        if 'value' in x_col.lower() or 'zhvi' in x_col.lower():\n",
    "            plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n",
    "        \n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(x_label, fontsize=12)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_summary_table(df, sort_col, columns_to_show, title):\n",
    "        \"\"\"Create a formatted summary table\"\"\"\n",
    "        display_df = df.sort_values(sort_col)[columns_to_show].copy()\n",
    "        print(f\"\\nüìã {title}\")\n",
    "        print(\"=\" * 80)\n",
    "        display(display_df)\n",
    "        return display_df\n",
    "\n",
    "# Initialize analysis utils\n",
    "analysis_utils = AnalysisUtils()\n",
    "print(\"‚úÖ Analysis utilities initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Main Data Collection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect Census demographic data\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è Starting data collection pipeline...\\n\")\n",
    "\n",
    "census_df = data_collector.collect_census_data()\n",
    "census_df.to_csv(f\"{Config.DATA_DIR}/census_zip_data.csv\", index=False)\n",
    "print(f\"üíæ Saved census data to {Config.DATA_DIR}/census_zip_data.csv\\n\")\n",
    "\n",
    "print(\"üìä Census Data Preview:\")\n",
    "display(census_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Zillow ZHVI data\n",
    "zillow_df = data_collector.load_zillow_data(Config.ZILLOW_FILE)\n",
    "\n",
    "if not zillow_df.empty:\n",
    "    print(\"\\nüè† Zillow Data Preview:\")\n",
    "    display(zillow_df.head())\n",
    "    \n",
    "    # Show top expensive areas\n",
    "    zillow_df['rank'] = zillow_df['zhvi_latest'].rank(ascending=False, method='min').
